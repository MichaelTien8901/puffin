{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline & Alpha Factors - Chapters 02-05\n",
    "\n",
    "This notebook demonstrates the Puffin data pipeline, covering the full workflow from\n",
    "data acquisition through alpha factor computation to portfolio optimization.\n",
    "\n",
    "**Chapters covered:**\n",
    "- **Ch 02 - Data Pipeline**: Data providers, caching, storage, and preprocessing\n",
    "- **Ch 03 - Alternative Data**: Web scraping, transcript parsing (not covered here)\n",
    "- **Ch 04 - Alpha Factors**: Momentum, volatility, technical indicators, Kalman filter, factor evaluation\n",
    "- **Ch 05 - Portfolio Optimization**: Mean-variance, risk parity, hierarchical risk parity, tearsheets\n",
    "\n",
    "**Key concepts:**\n",
    "- Strategy pattern for swappable `DataProvider` implementations\n",
    "- SQLite caching to avoid redundant API calls\n",
    "- Parquet/HDF5 persistent storage via `MarketDataStore`\n",
    "- Multi-horizon momentum and volatility factor construction\n",
    "- Kalman filter for signal denoising and trend extraction\n",
    "- Mean-variance, risk parity, and HRP portfolio construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Providers\n",
    "\n",
    "Puffin uses a `DataProvider` abstract base class with concrete implementations for different\n",
    "data sources. `YFinanceProvider` fetches free historical OHLCV data from Yahoo Finance.\n",
    "The strategy pattern means you can swap in `AlpacaProvider` or `IBKRDataProvider` without\n",
    "changing downstream code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.data import DataProvider, YFinanceProvider\n",
    "\n",
    "# YFinanceProvider implements the DataProvider interface\n",
    "provider = YFinanceProvider()\n",
    "print(f\"Supported assets: {provider.get_supported_assets()}\")\n",
    "\n",
    "# Fetch historical daily data for AAPL\n",
    "aapl = provider.fetch_historical(\"AAPL\", start=\"2022-01-01\", end=\"2024-01-01\")\n",
    "print(f\"\\nShape: {aapl.shape}\")\n",
    "print(f\"Index levels: {aapl.index.names}\")\n",
    "print(f\"Columns: {list(aapl.columns)}\")\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Ticker Download\n",
    "\n",
    "The same `fetch_historical` method handles multiple symbols, returning a MultiIndex\n",
    "DataFrame indexed by `(Date, Symbol)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a small universe for portfolio construction later\n",
    "symbols = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\"]\n",
    "multi = provider.fetch_historical(symbols, start=\"2022-01-01\", end=\"2024-01-01\")\n",
    "print(f\"Multi-ticker shape: {multi.shape}\")\n",
    "print(f\"Symbols: {multi.index.get_level_values('Symbol').unique().tolist()}\")\n",
    "multi.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Caching\n",
    "\n",
    "`DataCache` stores OHLCV data in a local SQLite database so that repeated fetches\n",
    "for the same symbol and date range hit the cache instead of the API. This is essential\n",
    "for backtesting workflows where you repeatedly iterate on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, os\n",
    "from puffin.data import DataCache, MarketDataStore\n",
    "\n",
    "# Create a temporary cache for demonstration\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "cache = DataCache(db_path=os.path.join(tmp_dir, \"demo_cache.db\"))\n",
    "\n",
    "# Extract single-symbol data (drop the Symbol level for cache compatibility)\n",
    "aapl_single = aapl.droplevel(\"Symbol\")\n",
    "\n",
    "# Store in cache\n",
    "cache.put(\"AAPL\", aapl_single, interval=\"1d\")\n",
    "print(\"Stored AAPL data in SQLite cache\")\n",
    "\n",
    "# Retrieve from cache\n",
    "cached = cache.get(\"AAPL\", start=\"2023-06-01\", end=\"2023-12-31\", interval=\"1d\")\n",
    "print(f\"Retrieved {len(cached)} rows from cache\")\n",
    "cached.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistent Storage with MarketDataStore\n",
    "\n",
    "`MarketDataStore` provides Parquet or HDF5 file-based storage with metadata tracking.\n",
    "This is more suitable for large datasets that need to persist across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MarketDataStore in Parquet format\n",
    "store_dir = os.path.join(tmp_dir, \"market_store\")\n",
    "store = MarketDataStore(store_dir, format=\"parquet\")\n",
    "\n",
    "# Save OHLCV data with metadata\n",
    "store.save_ohlcv(\"AAPL\", aapl_single, source=\"yfinance\", frequency=\"1d\")\n",
    "print(f\"Stored symbols: {store.list_symbols()}\")\n",
    "\n",
    "# Inspect metadata\n",
    "print(\"\\nMetadata:\")\n",
    "store.get_metadata(\"AAPL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "The `preprocess()` function handles missing values (forward-fill, interpolation, or drop),\n",
    "clips extreme return outliers, and validates OHLCV constraints (e.g., High >= Low,\n",
    "non-negative prices and volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from puffin.data import preprocess\n",
    "\n",
    "# Demonstrate preprocessing on data with injected issues\n",
    "dirty = aapl_single.copy()\n",
    "dirty.iloc[10:13, dirty.columns.get_loc(\"Close\")] = np.nan  # inject NaN\n",
    "dirty.iloc[50, dirty.columns.get_loc(\"Volume\")] = -100       # inject negative volume\n",
    "\n",
    "print(f\"Before preprocessing:\")\n",
    "print(f\"  NaN count: {dirty.isna().sum().sum()}\")\n",
    "print(f\"  Negative volume rows: {(dirty['Volume'] < 0).sum()}\")\n",
    "\n",
    "clean = preprocess(dirty, fill_method=\"ffill\", remove_outliers=True, outlier_std=5.0)\n",
    "\n",
    "print(f\"\\nAfter preprocessing:\")\n",
    "print(f\"  NaN count: {clean.isna().sum().sum()}\")\n",
    "print(f\"  Negative volume rows: {(clean['Volume'] < 0).sum()}\")\n",
    "print(f\"  Shape preserved: {dirty.shape} -> {clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alpha Factors\n",
    "\n",
    "Alpha factors quantify signals that predict future returns. Puffin provides:\n",
    "- **Momentum factors**: Returns over multiple horizons (5d, 21d, 63d, 252d)\n",
    "- **Volatility factors**: Realized, Parkinson, and Garman-Klass estimators\n",
    "- **`compute_all_factors()`**: Computes momentum + volatility in one call\n",
    "\n",
    "Factors are returned with a `(date, symbol)` MultiIndex for cross-sectional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.factors import (\n",
    "    compute_momentum_factors,\n",
    "    compute_volatility_factors,\n",
    "    compute_all_factors,\n",
    ")\n",
    "\n",
    "# Pivot multi-ticker data to wide format for factor computation\n",
    "close_prices = multi[\"Close\"].unstack(\"Symbol\")\n",
    "print(f\"Close prices shape: {close_prices.shape}\")\n",
    "\n",
    "# Compute momentum factors\n",
    "mom_factors = compute_momentum_factors(close_prices, windows=[5, 21, 63])\n",
    "print(f\"\\nMomentum factors shape: {mom_factors.shape}\")\n",
    "print(f\"Columns: {list(mom_factors.columns)}\")\n",
    "mom_factors.dropna().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute volatility factors\n",
    "vol_factors = compute_volatility_factors(close_prices, windows=[21, 63])\n",
    "print(f\"Volatility factors shape: {vol_factors.shape}\")\n",
    "print(f\"Columns: {list(vol_factors.columns)}\")\n",
    "vol_factors.dropna().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all factors at once\n",
    "all_factors = compute_all_factors(\n",
    "    close_prices,\n",
    "    momentum_windows=[5, 21, 63],\n",
    "    volatility_windows=[21, 63],\n",
    ")\n",
    "print(f\"Combined factors shape: {all_factors.shape}\")\n",
    "print(f\"All columns: {list(all_factors.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Technical Indicators\n",
    "\n",
    "`TechnicalIndicators` provides a unified interface for computing overlap studies (SMA, EMA,\n",
    "Bollinger Bands), momentum indicators (RSI, MACD, Stochastic), volume indicators (OBV, A/D),\n",
    "and volatility indicators (ATR). It uses TA-Lib when available, with pure Python fallbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.factors import TechnicalIndicators\n",
    "\n",
    "# Prepare OHLCV dict for a single symbol\n",
    "ohlcv = {\n",
    "    \"open\": aapl_single[\"Open\"],\n",
    "    \"high\": aapl_single[\"High\"],\n",
    "    \"low\": aapl_single[\"Low\"],\n",
    "    \"close\": aapl_single[\"Close\"],\n",
    "    \"volume\": aapl_single[\"Volume\"],\n",
    "}\n",
    "\n",
    "ti = TechnicalIndicators()\n",
    "indicators = ti.compute_all(ohlcv, categories=[\"overlap\", \"momentum\"])\n",
    "print(f\"Technical indicators shape: {indicators.shape}\")\n",
    "print(f\"Columns: {list(indicators.columns)}\")\n",
    "indicators[[\"sma_20\", \"sma_50\", \"rsi\", \"macd\"]].dropna().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Kalman Filter for Trend Extraction\n",
    "\n",
    "The `KalmanFilter` provides optimal recursive estimation for:\n",
    "- **Signal denoising**: Removing noise from price series\n",
    "- **Trend extraction**: Isolating the underlying trend via forward-backward smoothing\n",
    "- **Dynamic hedge ratios**: Time-varying beta estimation for pairs trading\n",
    "\n",
    "The `extract_trend()` convenience function applies a Kalman smoother with configurable\n",
    "process and observation noise parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from puffin.factors import KalmanFilter, extract_trend\n",
    "\n",
    "close = aapl_single[\"Close\"]\n",
    "\n",
    "# Extract trend with different smoothing levels\n",
    "trend_smooth = extract_trend(close, process_variance=1e-6, observation_variance=1e-1)\n",
    "trend_responsive = extract_trend(close, process_variance=1e-3, observation_variance=1e-1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "close.plot(ax=ax, alpha=0.5, label=\"Raw Close\")\n",
    "trend_smooth.plot(ax=ax, label=\"Smooth trend (Q=1e-6)\", linewidth=2)\n",
    "trend_responsive.plot(ax=ax, label=\"Responsive trend (Q=1e-3)\", linewidth=2)\n",
    "ax.set_title(\"Kalman Filter Trend Extraction - AAPL\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Factor Evaluation\n",
    "\n",
    "`FactorEvaluator` measures the predictive power of alpha factors using:\n",
    "- **Information Coefficient (IC)**: Correlation between factor values and forward returns\n",
    "- **Factor returns**: Long-short portfolio returns by quantile\n",
    "- **Turnover analysis**: How often factor rankings change\n",
    "- **Full tearsheet**: All metrics combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.factors import FactorEvaluator\n",
    "\n",
    "# Prepare a momentum factor for evaluation\n",
    "# The factor must be a Series with (date, asset) MultiIndex\n",
    "mom_21 = mom_factors[\"mom_21\"].dropna()\n",
    "print(f\"Factor shape: {mom_21.shape}\")\n",
    "print(f\"Index names: {mom_21.index.names}\")\n",
    "\n",
    "# Evaluate the 21-day momentum factor\n",
    "evaluator = FactorEvaluator(quantiles=5, periods=[1, 5, 21])\n",
    "\n",
    "# Compute Information Coefficient\n",
    "forward_returns_1d = close_prices.pct_change(1).shift(-1)\n",
    "ic = evaluator.compute_ic(mom_21, forward_returns_1d, method=\"spearman\")\n",
    "print(f\"\\nMean IC (Spearman): {ic.mean():.4f}\")\n",
    "print(f\"IC Std: {ic.std():.4f}\")\n",
    "print(f\"IC IR (IC Mean / IC Std): {ic.mean() / (ic.std() + 1e-8):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Portfolio Optimization\n",
    "\n",
    "Puffin implements three portfolio construction methods:\n",
    "- **Mean-Variance (Markowitz)**: Classic optimization maximizing Sharpe ratio or minimizing variance\n",
    "- **Risk Parity**: Each asset contributes equally to total portfolio risk\n",
    "- **Hierarchical Risk Parity (HRP)**: Clustering-based allocation for more stable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.portfolio import MeanVarianceOptimizer, risk_parity_weights, hrp_weights\n",
    "\n",
    "# Compute daily returns for the universe\n",
    "returns_df = close_prices.pct_change().dropna()\n",
    "\n",
    "# --- Mean-Variance: Maximum Sharpe Ratio ---\n",
    "mvo = MeanVarianceOptimizer()\n",
    "max_sharpe = mvo.max_sharpe(returns_df)\n",
    "print(\"=== Max Sharpe Portfolio ===\")\n",
    "for sym, w in zip(returns_df.columns, max_sharpe[\"weights\"]):\n",
    "    print(f\"  {sym}: {w:.2%}\")\n",
    "print(f\"  Expected Return: {max_sharpe['return'] * 252:.2%}\")\n",
    "print(f\"  Volatility: {max_sharpe['risk'] * np.sqrt(252):.2%}\")\n",
    "print(f\"  Sharpe: {max_sharpe['sharpe']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Risk Parity ---\n",
    "rp_weights = risk_parity_weights(returns_df)\n",
    "print(\"=== Risk Parity Portfolio ===\")\n",
    "for sym, w in zip(returns_df.columns, rp_weights):\n",
    "    print(f\"  {sym}: {w:.2%}\")\n",
    "\n",
    "# --- Hierarchical Risk Parity ---\n",
    "hrp_w = hrp_weights(returns_df, linkage_method=\"single\")\n",
    "print(\"\\n=== HRP Portfolio ===\")\n",
    "for sym, w in zip(returns_df.columns, hrp_w):\n",
    "    print(f\"  {sym}: {w:.2%}\")\n",
    "\n",
    "# Compare all three methods\n",
    "comparison = pd.DataFrame({\n",
    "    \"Max Sharpe\": max_sharpe[\"weights\"],\n",
    "    \"Risk Parity\": rp_weights,\n",
    "    \"HRP\": hrp_w,\n",
    "}, index=returns_df.columns)\n",
    "print(\"\\n=== Weight Comparison ===\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Tearsheet\n",
    "\n",
    "`generate_tearsheet()` computes comprehensive portfolio statistics including annualized\n",
    "return, Sharpe ratio, Sortino ratio, maximum drawdown, VaR/CVaR, and win rate.\n",
    "`plot_returns()` visualizes cumulative performance against an optional benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.portfolio import generate_tearsheet, plot_returns, print_tearsheet_summary\n",
    "\n",
    "# Simulate portfolio returns using HRP weights\n",
    "portfolio_returns = (returns_df * hrp_w).sum(axis=1)\n",
    "portfolio_returns.name = \"HRP Portfolio\"\n",
    "\n",
    "# Equal-weight benchmark\n",
    "equal_weights = np.ones(len(symbols)) / len(symbols)\n",
    "benchmark_returns = (returns_df * equal_weights).sum(axis=1)\n",
    "benchmark_returns.name = \"Equal Weight\"\n",
    "\n",
    "# Generate tearsheet\n",
    "tearsheet = generate_tearsheet(portfolio_returns, benchmark=benchmark_returns)\n",
    "print_tearsheet_summary(tearsheet)\n",
    "\n",
    "# Plot cumulative returns\n",
    "fig = plot_returns(portfolio_returns, benchmark=benchmark_returns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Expand the universe**: Add 5 more symbols and re-run the portfolio optimization. How do the HRP weights change?\n",
    "2. **Factor decay**: Use `FactorEvaluator.compute_factor_returns()` with periods `[1, 5, 21]` to see how momentum factor returns decay over longer horizons.\n",
    "3. **Kalman crossover**: Use `kalman_ma_crossover()` from `puffin.factors` to generate trading signals and backtest the strategy.\n",
    "4. **Compare volatility estimators**: Compute Parkinson and Garman-Klass volatility by passing OHLC data as a dict to `compute_volatility_factors()`. Plot all three estimators against each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
