{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# ML Models for Trading - Chapters 08-12\n",
    "\n",
    "This notebook covers the machine learning cluster from the Puffin project, spanning:\n",
    "\n",
    "- **Chapter 08**: Linear models (OLS, Ridge, Lasso, Logistic Regression, Fama-French)\n",
    "- **Chapter 09**: Time series models (stationarity, ARIMA, VAR, GARCH, cointegration)\n",
    "- **Chapter 10**: Bayesian ML (covered separately)\n",
    "- **Chapter 11**: Tree ensembles (Random Forest, XGBoost, LightGBM, SHAP)\n",
    "- **Chapter 12**: Unsupervised learning (PCA, clustering, data-driven risk factors)\n",
    "\n",
    "Each section imports from the `puffin` package and demonstrates the model APIs with synthetic or downloaded data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-models-md",
   "metadata": {},
   "source": [
    "## 1. Linear Models\n",
    "\n",
    "Linear regression models form the baseline for return prediction and factor analysis.\n",
    "We demonstrate OLS, Ridge (L2), Lasso (L1), and a direction classifier (logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-models-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from puffin.models import OLSModel, RidgeModel, LassoModel, DirectionClassifier\n",
    "\n",
    "# Generate synthetic return prediction data\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "dates = pd.date_range('2020-01-01', periods=n, freq='B')\n",
    "\n",
    "features = pd.DataFrame({\n",
    "    'momentum_5d': np.random.randn(n) * 0.02,\n",
    "    'momentum_20d': np.random.randn(n) * 0.03,\n",
    "    'volatility_20d': np.abs(np.random.randn(n)) * 0.01,\n",
    "    'volume_ratio': 1 + np.random.randn(n) * 0.3,\n",
    "    'rsi_14': 50 + np.random.randn(n) * 15,\n",
    "}, index=dates)\n",
    "\n",
    "# Target: next-day returns with some signal from features\n",
    "returns = (0.3 * features['momentum_5d'] + 0.2 * features['momentum_20d']\n",
    "           - 0.1 * features['volatility_20d'] + np.random.randn(n) * 0.01)\n",
    "returns.name = 'forward_return'\n",
    "\n",
    "# Train/test split (80/20)\n",
    "split = int(n * 0.8)\n",
    "X_train, X_test = features.iloc[:split], features.iloc[split:]\n",
    "y_train, y_test = returns.iloc[:split], returns.iloc[split:]\n",
    "\n",
    "# --- OLS ---\n",
    "ols = OLSModel().fit(X_train, y_train)\n",
    "ols_summary = ols.summary()\n",
    "print(\"=== OLS Model ===\")\n",
    "print(f\"R-squared: {ols_summary['r_squared']:.4f}\")\n",
    "print(f\"Significant features (p < 0.05):\")\n",
    "for name, pval in ols.p_values.items():\n",
    "    if pval < 0.05:\n",
    "        print(f\"  {name}: coef={ols.coefficients[name]:.6f}, p={pval:.4f}\")\n",
    "\n",
    "# --- Ridge ---\n",
    "ridge = RidgeModel().fit(X_train, y_train)\n",
    "print(f\"\\n=== Ridge Model (alpha={ridge.alpha:.4f}) ===\")\n",
    "print(\"Feature importance:\")\n",
    "print(ridge.feature_importance().head())\n",
    "\n",
    "# --- Lasso ---\n",
    "lasso = LassoModel().fit(X_train, y_train)\n",
    "print(f\"\\n=== Lasso Model (alpha={lasso.alpha:.6f}) ===\")\n",
    "print(f\"Selected features: {lasso.selected_features}\")\n",
    "print(f\"Coefficients:\\n{lasso.coefficients}\")\n",
    "\n",
    "# --- Direction Classifier ---\n",
    "direction = (y_train > 0).astype(int)\n",
    "clf = DirectionClassifier().fit(X_train, direction)\n",
    "test_direction = (y_test > 0).astype(int)\n",
    "preds = clf.predict(X_test)\n",
    "accuracy = (preds == test_direction.values).mean()\n",
    "print(f\"\\n=== Direction Classifier ===\")\n",
    "print(f\"Test accuracy: {accuracy:.2%}\")\n",
    "print(f\"Feature importance:\\n{clf.feature_importance()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fama-french-md",
   "metadata": {},
   "source": [
    "## 2. Fama-French Factor Models\n",
    "\n",
    "The Fama-French model explains asset returns using common risk factors: market, size (SMB),\n",
    "value (HML), profitability (RMW), and investment (CMA). We fit CAPM, 3-factor, and 5-factor models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fama-french-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.models import FamaFrenchModel\n",
    "\n",
    "ff = FamaFrenchModel()\n",
    "\n",
    "# Generate synthetic asset returns for demonstration\n",
    "np.random.seed(42)\n",
    "n_days = 504  # ~2 years of trading days\n",
    "dates = pd.date_range('2022-01-01', periods=n_days, freq='B')\n",
    "\n",
    "# Simulate returns with known factor exposures\n",
    "factors = ff.fetch_factors('2022-01-01', '2024-01-01')\n",
    "factors = factors.iloc[:n_days]  # Align length\n",
    "factors.index = dates\n",
    "\n",
    "# Asset with beta=1.2, positive SMB, negative HML\n",
    "asset_returns = pd.Series(\n",
    "    0.0002 + 1.2 * factors['Mkt-RF'] + 0.5 * factors['SMB']\n",
    "    - 0.3 * factors['HML'] + factors['RF'] + np.random.randn(n_days) * 0.005,\n",
    "    index=dates\n",
    ")\n",
    "\n",
    "# CAPM\n",
    "capm = ff.fit_capm(asset_returns, start='2022-01-01', end='2024-01-01')\n",
    "print(\"=== CAPM ===\")\n",
    "print(f\"Alpha: {capm['alpha']:.6f} (p={capm['alpha_pvalue']:.4f})\")\n",
    "print(f\"Beta:  {capm['beta']:.4f} (p={capm['beta_pvalue']:.4f})\")\n",
    "print(f\"R-squared: {capm['r_squared']:.4f}\")\n",
    "\n",
    "# 3-Factor Model\n",
    "ff3 = ff.fit_three_factor(asset_returns, start='2022-01-01', end='2024-01-01')\n",
    "print(f\"\\n=== Fama-French 3-Factor ===\")\n",
    "print(f\"Alpha: {ff3['alpha']:.6f}\")\n",
    "for factor, beta in ff3['betas'].items():\n",
    "    pval = ff3['pvalues'][factor]\n",
    "    print(f\"  {factor}: beta={beta:.4f} (p={pval:.4f})\")\n",
    "print(f\"R-squared: {ff3['r_squared']:.4f}\")\n",
    "\n",
    "# 5-Factor Model\n",
    "ff5 = ff.fit_five_factor(asset_returns, start='2022-01-01', end='2024-01-01')\n",
    "print(f\"\\n=== Fama-French 5-Factor ===\")\n",
    "print(f\"Alpha: {ff5['alpha']:.6f}\")\n",
    "for factor, beta in ff5['betas'].items():\n",
    "    pval = ff5['pvalues'][factor]\n",
    "    print(f\"  {factor}: beta={beta:.4f} (p={pval:.4f})\")\n",
    "print(f\"R-squared: {ff5['r_squared']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ts-diagnostics-md",
   "metadata": {},
   "source": [
    "## 3. Time Series Diagnostics\n",
    "\n",
    "Before fitting time series models, we must check for stationarity and decompose the series\n",
    "into trend, seasonal, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ts-diagnostics-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.models import test_stationarity, decompose_series\n",
    "\n",
    "# Generate a non-stationary price series and its stationary returns\n",
    "np.random.seed(42)\n",
    "n = 600\n",
    "dates = pd.date_range('2020-01-01', periods=n, freq='B')\n",
    "returns_series = pd.Series(np.random.randn(n) * 0.01 + 0.0003, index=dates)\n",
    "prices = (1 + returns_series).cumprod() * 100\n",
    "\n",
    "# Test stationarity on prices vs returns\n",
    "print(\"=== Prices (expected: non-stationary) ===\")\n",
    "price_result = test_stationarity(prices)\n",
    "print(f\"ADF statistic: {price_result['test_statistic']:.4f}\")\n",
    "print(f\"p-value: {price_result['p_value']:.4f}\")\n",
    "print(f\"Is stationary: {price_result['is_stationary']}\")\n",
    "\n",
    "print(f\"\\n=== Returns (expected: stationary) ===\")\n",
    "ret_result = test_stationarity(returns_series)\n",
    "print(f\"ADF statistic: {ret_result['test_statistic']:.4f}\")\n",
    "print(f\"p-value: {ret_result['p_value']:.4f}\")\n",
    "print(f\"Is stationary: {ret_result['is_stationary']}\")\n",
    "\n",
    "# Decompose the price series\n",
    "components = decompose_series(prices, period=63)  # Quarterly seasonality\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
    "for ax, (name, data) in zip(axes, components.items()):\n",
    "    ax.plot(data)\n",
    "    ax.set_ylabel(name.capitalize())\n",
    "axes[0].set_title('Time Series Decomposition')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arima-md",
   "metadata": {},
   "source": [
    "## 4. ARIMA Models\n",
    "\n",
    "ARIMA models combine autoregressive (AR), differencing (I), and moving average (MA) components.\n",
    "We fit a manual ARIMA and use `auto_arima` for automatic order selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arima-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.models import ARIMAModel, auto_arima\n",
    "\n",
    "# Use the returns series from above\n",
    "train_returns = returns_series.iloc[:480]\n",
    "test_returns = returns_series.iloc[480:]\n",
    "\n",
    "# Manual ARIMA(1,0,1) fit\n",
    "model = ARIMAModel(order=(1, 0, 1))\n",
    "model.fit(train_returns)\n",
    "print(f\"=== ARIMA(1,0,1) ===\")\n",
    "print(f\"AIC: {model.aic_:.2f}\")\n",
    "print(f\"BIC: {model.bic_:.2f}\")\n",
    "\n",
    "# Forecast with confidence intervals\n",
    "forecast_df = model.forecast(train_returns, horizon=20, confidence=0.95)\n",
    "print(f\"\\n20-step forecast (first 5 rows):\")\n",
    "print(forecast_df.head())\n",
    "\n",
    "# Auto-ARIMA: search for optimal order\n",
    "auto_model = auto_arima(train_returns, max_p=3, max_d=1, max_q=3)\n",
    "print(f\"\\n=== Auto ARIMA ===\")\n",
    "print(f\"Selected order: {auto_model.order_}\")\n",
    "print(f\"AIC: {auto_model.aic_:.2f}\")\n",
    "\n",
    "# Plot forecast\n",
    "forecast_df = auto_model.forecast(train_returns, horizon=30)\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "train_returns.iloc[-60:].plot(ax=ax, label='Historical')\n",
    "forecast_df['forecast'].plot(ax=ax, label='Forecast', color='red')\n",
    "ax.fill_between(forecast_df.index, forecast_df['lower'], forecast_df['upper'],\n",
    "                alpha=0.2, color='red', label='95% CI')\n",
    "ax.legend()\n",
    "ax.set_title(f'ARIMA{auto_model.order_} Forecast')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "var-garch-md",
   "metadata": {},
   "source": [
    "## 5. VAR & GARCH Models\n",
    "\n",
    "**VAR** captures linear interdependencies among multiple time series.\n",
    "**GARCH** models time-varying volatility and volatility clustering in financial returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "var-garch-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.models import VARModel, GARCHModel\n",
    "\n",
    "# --- VAR Model ---\n",
    "# Simulate two correlated return series\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "dates = pd.date_range('2020-01-01', periods=n, freq='B')\n",
    "eps1 = np.random.randn(n) * 0.01\n",
    "eps2 = np.random.randn(n) * 0.01\n",
    "r1 = np.zeros(n)\n",
    "r2 = np.zeros(n)\n",
    "for t in range(1, n):\n",
    "    r1[t] = 0.3 * r1[t-1] + 0.1 * r2[t-1] + eps1[t]\n",
    "    r2[t] = 0.05 * r1[t-1] + 0.4 * r2[t-1] + eps2[t]\n",
    "\n",
    "var_data = pd.DataFrame({'stock_A': r1, 'stock_B': r2}, index=dates)\n",
    "\n",
    "var_model = VARModel()\n",
    "var_model.fit(var_data, max_lags=5)\n",
    "print(f\"=== VAR Model (lags={var_model.lags_}) ===\")\n",
    "\n",
    "# Forecast\n",
    "var_forecast = var_model.predict(steps=5)\n",
    "print(f\"5-step forecast:\\n{var_forecast}\")\n",
    "\n",
    "# Impulse response\n",
    "irf = var_model.impulse_response(periods=20)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "axes[0].plot(irf[:, 0, 0], label='A -> A')\n",
    "axes[0].plot(irf[:, 1, 0], label='A -> B')\n",
    "axes[0].set_title('Impulse Response: Shock to Stock A')\n",
    "axes[0].legend()\n",
    "axes[1].plot(irf[:, 0, 1], label='B -> A')\n",
    "axes[1].plot(irf[:, 1, 1], label='B -> B')\n",
    "axes[1].set_title('Impulse Response: Shock to Stock B')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- GARCH Model ---\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "dates = pd.date_range('2018-01-01', periods=n, freq='B')\n",
    "returns_garch = pd.Series(np.random.randn(n) * 0.015, index=dates)\n",
    "# Add volatility clustering\n",
    "for t in range(1, n):\n",
    "    vol = 0.005 + 0.7 * abs(returns_garch.iloc[t-1])\n",
    "    returns_garch.iloc[t] = np.random.randn() * vol\n",
    "\n",
    "garch = GARCHModel(p=1, q=1)\n",
    "garch.fit(returns_garch * 100)  # Scale to percentage\n",
    "print(f\"\\n=== GARCH(1,1) ===\")\n",
    "params = garch.get_params()\n",
    "print(f\"AIC: {params['aic']:.2f}\")\n",
    "print(f\"Parameters:\\n{params['params']}\")\n",
    "\n",
    "# Plot conditional volatility\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.plot(garch.conditional_volatility, label='Conditional Volatility')\n",
    "ax.set_title('GARCH(1,1) Conditional Volatility')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cointegration-md",
   "metadata": {},
   "source": [
    "## 6. Cointegration & Pairs Trading\n",
    "\n",
    "Cointegration identifies long-run equilibrium relationships between price series.\n",
    "We use it to build a pairs trading strategy that profits from mean-reverting spreads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cointegration-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.models import find_cointegrated_pairs, PairsTradingStrategy\n",
    "\n",
    "# Simulate cointegrated pairs\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "dates = pd.date_range('2020-01-01', periods=n, freq='B')\n",
    "\n",
    "# Cointegrated pair: stock_A and stock_B share a common stochastic trend\n",
    "common_trend = np.random.randn(n).cumsum()\n",
    "stock_A = 100 + common_trend + np.random.randn(n) * 0.5\n",
    "stock_B = 50 + 0.5 * common_trend + np.random.randn(n) * 0.3\n",
    "\n",
    "# Non-cointegrated stock\n",
    "stock_C = 80 + np.random.randn(n).cumsum()\n",
    "\n",
    "prices = pd.DataFrame({\n",
    "    'stock_A': stock_A,\n",
    "    'stock_B': stock_B,\n",
    "    'stock_C': stock_C,\n",
    "}, index=dates)\n",
    "\n",
    "# Find cointegrated pairs\n",
    "pairs = find_cointegrated_pairs(prices, significance=0.05)\n",
    "print(\"=== Cointegrated Pairs ===\")\n",
    "for t1, t2, pval, hr in pairs:\n",
    "    print(f\"  {t1} - {t2}: p-value={pval:.4f}, hedge_ratio={hr:.4f}\")\n",
    "\n",
    "# Pairs trading strategy\n",
    "strategy = PairsTradingStrategy(entry_z=2.0, exit_z=0.5, lookback=20)\n",
    "\n",
    "if pairs:\n",
    "    best_pair = (pairs[0][0], pairs[0][1])\n",
    "    hedge_ratio = pairs[0][3]\n",
    "\n",
    "    # Compute spread and signals\n",
    "    spread = strategy.compute_spread(best_pair, prices, hedge_ratio=hedge_ratio)\n",
    "    signals = strategy.generate_signals(spread)\n",
    "\n",
    "    # Backtest\n",
    "    results = strategy.backtest_pair(best_pair, prices, hedge_ratio=hedge_ratio)\n",
    "    print(f\"\\n=== Pairs Trading Backtest: {best_pair[0]}-{best_pair[1]} ===\")\n",
    "    print(f\"Sharpe Ratio: {results['sharpe_ratio']:.2f}\")\n",
    "    print(f\"Total Return: {results['total_return']:.2%}\")\n",
    "    print(f\"Max Drawdown: {results['max_drawdown']:.2%}\")\n",
    "    print(f\"Win Rate: {results['win_rate']:.2%}\")\n",
    "    print(f\"Number of Trades: {results['num_trades']}\")\n",
    "\n",
    "    # Plot spread and signals\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "    spread.plot(ax=axes[0], label='Spread')\n",
    "    mean = spread.rolling(20).mean()\n",
    "    std = spread.rolling(20).std()\n",
    "    axes[0].plot(mean, 'r--', label='Mean')\n",
    "    axes[0].fill_between(spread.index, mean - 2*std, mean + 2*std, alpha=0.1, color='red')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title('Spread with Bollinger Bands')\n",
    "\n",
    "    results['cumulative_returns'].plot(ax=axes[1], label='Strategy')\n",
    "    axes[1].set_title('Cumulative Returns')\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tree-ensembles-md",
   "metadata": {},
   "source": [
    "## 7. Tree Ensemble Models\n",
    "\n",
    "Tree ensembles aggregate many decision trees for robust predictions.\n",
    "We compare Random Forest, XGBoost, and LightGBM on a directional trading task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tree-ensembles-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.ensembles import RandomForestTrader, XGBoostTrader, LightGBMTrader\n",
    "\n",
    "# Create feature set for direction prediction\n",
    "np.random.seed(42)\n",
    "n = 800\n",
    "dates = pd.date_range('2019-01-01', periods=n, freq='B')\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    'mom_5': np.random.randn(n) * 0.02,\n",
    "    'mom_10': np.random.randn(n) * 0.025,\n",
    "    'mom_20': np.random.randn(n) * 0.03,\n",
    "    'vol_10': np.abs(np.random.randn(n)) * 0.01,\n",
    "    'vol_20': np.abs(np.random.randn(n)) * 0.012,\n",
    "    'rsi': 50 + np.random.randn(n) * 15,\n",
    "    'macd': np.random.randn(n) * 0.005,\n",
    "    'bb_width': np.abs(np.random.randn(n)) * 0.02,\n",
    "}, index=dates)\n",
    "\n",
    "# Generate labels with some predictability\n",
    "signal = 0.3 * X['mom_5'] + 0.2 * X['macd'] - 0.1 * X['vol_20']\n",
    "y = (signal + np.random.randn(n) * 0.005 > 0).astype(int)\n",
    "y = pd.Series(y, index=dates, name='direction')\n",
    "\n",
    "split = int(n * 0.75)\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestTrader(task='classification')\n",
    "rf.fit(X_train, y_train, n_estimators=200, max_depth=8)\n",
    "rf_preds = rf.predict(X_test)\n",
    "rf_acc = np.nanmean(rf_preds == y_test.values)\n",
    "results['RandomForest'] = rf_acc\n",
    "print(f\"Random Forest accuracy: {rf_acc:.2%}\")\n",
    "\n",
    "# XGBoost\n",
    "if XGBoostTrader is not None:\n",
    "    xgb_model = XGBoostTrader(task='classification')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_preds = xgb_model.predict(X_test)\n",
    "    xgb_acc = np.nanmean(xgb_preds == y_test.values)\n",
    "    results['XGBoost'] = xgb_acc\n",
    "    print(f\"XGBoost accuracy: {xgb_acc:.2%}\")\n",
    "else:\n",
    "    print(\"XGBoost not installed, skipping.\")\n",
    "\n",
    "# LightGBM\n",
    "if LightGBMTrader is not None:\n",
    "    lgb_model = LightGBMTrader(task='classification')\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_preds = lgb_model.predict(X_test)\n",
    "    lgb_acc = np.nanmean(lgb_preds == y_test.values)\n",
    "    results['LightGBM'] = lgb_acc\n",
    "    print(f\"LightGBM accuracy: {lgb_acc:.2%}\")\n",
    "else:\n",
    "    print(\"LightGBM not installed, skipping.\")\n",
    "\n",
    "# Compare feature importances\n",
    "print(\"\\n=== Feature Importance (Random Forest) ===\")\n",
    "print(rf.feature_importance())\n",
    "\n",
    "# Cross-validation\n",
    "cv_results = rf.cross_validate(X, y, n_estimators=100, max_depth=8)\n",
    "print(f\"\\nTime-series CV: mean={cv_results['mean_score']:.4f}, std={cv_results['std_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation-md",
   "metadata": {},
   "source": [
    "## 8. Model Interpretation with SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values explain individual predictions by attributing\n",
    "each feature's contribution. This is critical for understanding why a model generates a\n",
    "particular trading signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpretation-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.ensembles import ModelInterpreter\n",
    "\n",
    "if ModelInterpreter is not None:\n",
    "    interpreter = ModelInterpreter()\n",
    "\n",
    "    # Compute SHAP values for the Random Forest model\n",
    "    shap_vals = interpreter.shap_values(rf.model, X_test)\n",
    "    print(f\"SHAP values shape: {shap_vals.values.shape}\")\n",
    "    print(f\"Mean |SHAP| per feature:\")\n",
    "    mean_shap = pd.Series(\n",
    "        np.abs(shap_vals.values).mean(axis=0),\n",
    "        index=X_test.columns\n",
    "    ).sort_values(ascending=False)\n",
    "    print(mean_shap)\n",
    "\n",
    "    # Summary plot\n",
    "    fig = interpreter.plot_summary(rf.model, X_test, plot_type='bar', max_display=8)\n",
    "    plt.show()\n",
    "\n",
    "    # Waterfall plot for a single prediction\n",
    "    fig = interpreter.plot_waterfall(rf.model, X_test, index=0)\n",
    "    plt.show()\n",
    "\n",
    "    # Compare models if XGBoost is available\n",
    "    models_dict = {'RandomForest': rf.model}\n",
    "    if XGBoostTrader is not None:\n",
    "        models_dict['XGBoost'] = xgb_model.model\n",
    "    if LightGBMTrader is not None:\n",
    "        models_dict['LightGBM'] = lgb_model.model\n",
    "\n",
    "    if len(models_dict) > 1:\n",
    "        comparison = interpreter.feature_importance_comparison(models_dict, X_test, method='native')\n",
    "        print(\"\\n=== Feature Importance Comparison ===\")\n",
    "        print(comparison.head(8))\n",
    "else:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsupervised-md",
   "metadata": {},
   "source": [
    "## 9. Unsupervised Learning: PCA & Clustering\n",
    "\n",
    "PCA extracts eigenportfolios (statistical risk factors) from the cross-section of returns.\n",
    "Clustering groups assets with similar behavior, useful for sector rotation and diversification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsupervised-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.unsupervised import MarketPCA, cluster_assets\n",
    "\n",
    "# Simulate a universe of 20 assets with sector structure\n",
    "np.random.seed(42)\n",
    "n_days = 504\n",
    "n_assets = 20\n",
    "dates = pd.date_range('2022-01-01', periods=n_days, freq='B')\n",
    "\n",
    "# 3 hidden factors (market, sector1, sector2)\n",
    "market_factor = np.random.randn(n_days) * 0.01\n",
    "sector1_factor = np.random.randn(n_days) * 0.008\n",
    "sector2_factor = np.random.randn(n_days) * 0.006\n",
    "\n",
    "returns_matrix = np.zeros((n_days, n_assets))\n",
    "for i in range(n_assets):\n",
    "    beta_mkt = 0.8 + np.random.rand() * 0.4\n",
    "    beta_s1 = (0.5 if i < 7 else -0.3) * np.random.rand()\n",
    "    beta_s2 = (0.5 if 7 <= i < 14 else -0.2) * np.random.rand()\n",
    "    returns_matrix[:, i] = (\n",
    "        beta_mkt * market_factor + beta_s1 * sector1_factor\n",
    "        + beta_s2 * sector2_factor + np.random.randn(n_days) * 0.005\n",
    "    )\n",
    "\n",
    "tickers = [f'STOCK_{i:02d}' for i in range(n_assets)]\n",
    "returns_df = pd.DataFrame(returns_matrix, index=dates, columns=tickers)\n",
    "\n",
    "# --- PCA ---\n",
    "pca = MarketPCA()\n",
    "pca.fit(returns_df)\n",
    "print(\"=== PCA Analysis ===\")\n",
    "print(f\"Components for 95% variance: {pca.n_components_95}\")\n",
    "print(f\"Top 5 explained variance ratios: {pca.explained_variance_ratio[:5].round(4)}\")\n",
    "\n",
    "# Plot explained variance\n",
    "var_data = pca.explained_variance_plot()\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(var_data['component'], var_data['variance_explained'], alpha=0.6, label='Individual')\n",
    "ax.plot(var_data['component'], var_data['cumulative_variance'], 'r-o', label='Cumulative')\n",
    "ax.axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "ax.set_xlabel('Component')\n",
    "ax.set_ylabel('Variance Explained')\n",
    "ax.set_title('PCA Scree Plot')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenportfolios\n",
    "eigenportfolios = pca.eigenportfolios(returns_df, n=3)\n",
    "print(\"\\nTop 3 Eigenportfolio Weights:\")\n",
    "print(eigenportfolios.round(3))\n",
    "\n",
    "# --- Clustering ---\n",
    "labels = cluster_assets(returns_df, n_clusters=3, method='kmeans')\n",
    "print(f\"\\n=== Clustering ===\")\n",
    "for cluster_id in np.unique(labels):\n",
    "    assets_in_cluster = [tickers[i] for i in range(n_assets) if labels[i] == cluster_id]\n",
    "    print(f\"Cluster {cluster_id}: {assets_in_cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "risk-factors-md",
   "metadata": {},
   "source": [
    "## 10. Data-Driven Risk Factors\n",
    "\n",
    "We extract statistical risk factors from the cross-section of returns using PCA,\n",
    "then compute factor exposures (loadings) and attribute asset returns to these factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "risk-factors-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from puffin.unsupervised import extract_risk_factors, factor_attribution\n",
    "from puffin.unsupervised.risk_factors import factor_exposures, factor_variance_decomposition\n",
    "\n",
    "# Extract 5 risk factors from the returns universe\n",
    "factors = extract_risk_factors(returns_df, n_factors=5)\n",
    "print(\"=== Extracted Risk Factors ===\")\n",
    "print(f\"Shape: {factors.shape}\")\n",
    "print(f\"Factor correlations:\\n{factors.corr().round(3)}\")\n",
    "\n",
    "# Factor exposures (loadings)\n",
    "loadings = factor_exposures(returns_df, factors)\n",
    "print(f\"\\nFactor Loadings (first 5 assets):\")\n",
    "print(loadings.head().round(4))\n",
    "\n",
    "# Factor attribution\n",
    "attribution = factor_attribution(returns_df, factors, loadings)\n",
    "print(f\"\\nFactor-attributed returns shape: {attribution.shape}\")\n",
    "\n",
    "# Variance decomposition\n",
    "var_decomp = factor_variance_decomposition(returns_df, n_factors=5)\n",
    "print(f\"\\n=== Variance Decomposition (first 5 assets) ===\")\n",
    "print(var_decomp[['pct_factor', 'pct_specific']].head().round(1))\n",
    "\n",
    "# Plot variance decomposition\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "var_decomp[['pct_factor', 'pct_specific']].plot(\n",
    "    kind='bar', stacked=True, ax=ax, color=['steelblue', 'lightcoral']\n",
    ")\n",
    "ax.set_ylabel('Percentage of Total Variance')\n",
    "ax.set_title('Factor vs Specific Variance Decomposition')\n",
    "ax.set_xticklabels(var_decomp.index, rotation=45, ha='right')\n",
    "ax.legend(['Factor Risk', 'Specific Risk'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises-md",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Linear Models**: Replace synthetic data with real returns from `yfinance`. Compare OLS, Ridge, and Lasso R-squared on out-of-sample data. Which regularization helps most?\n",
    "2. **ARIMA**: Use `auto_arima` on VIX data. Does the selected order change across different time windows?\n",
    "3. **Pairs Trading**: Download prices for sector ETFs (XLF, XLK, XLE, etc.) and run `find_cointegrated_pairs`. Backtest the top pair.\n",
    "4. **Tree Ensembles**: Tune XGBoost hyperparameters with `tune_hyperparameters()`. Does the tuned model beat the default on walk-forward validation?\n",
    "5. **PCA**: Apply `MarketPCA` to S&P 500 constituents. How many components explain 90% of variance? Do eigenportfolios correspond to known sectors?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor_version": 4,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
